{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "# step 3\n",
    "from nilearn import datasets\n",
    "from nilearn import image, plotting\n",
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "from alive_progress import alive_bar\n",
    "from tqdm import tqdm\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from hm_tools import *\n",
    "import scipy.io \n",
    "import h5py\n",
    "import mne\n",
    "import sys\n",
    "import os\n",
    "from glob import glob\n",
    "sys.path.insert(0, 'E:/workspace/my_py_toolbox/')\n",
    "from hm_tools import *\n",
    "\n",
    "data_path = \"E:/workspace/OTface/roi_data/aal_smooth6/\"\n",
    "result_path = \"E:/workspace/OTface/roi_data/aal_smooth6/cpm/\"\n",
    "condition_name = ['OT', 'PL']\n",
    "file_name = ['task', 'rest']\n",
    "\n",
    "# mapping from subject id to group\n",
    "sub2group = dict()\n",
    "[sub2group.update({f\"s{path[-3:]}\": group}) for group in [\"OT\", \"PL\"] for path in glob(os.path.join(data_path, \"task\", \"preprocess\", group, \"sub*\"))]\n",
    "print(sub2group)\n",
    "\n",
    "behavior_df = pd.read_csv(\"E:/workspace/OTface/behavior_metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vlc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_shutdown(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90 aal rois\n",
    "roi_name = np.array(['Precentral_L','Precentral_R',\n",
    " 'Frontal_Sup_L', 'Frontal_Sup_R',\n",
    " 'Frontal_Sup_Orb_L', 'Frontal_Sup_Orb_R',\n",
    " 'Frontal_Mid_L', 'Frontal_Mid_R',\n",
    " 'Frontal_Mid_Orb_L', 'Frontal_Mid_Orb_R',\n",
    " 'Frontal_Inf_Oper_L', 'Frontal_Inf_Oper_R',\n",
    " 'Frontal_Inf_Tri_L', 'Frontal_Inf_Tri_R',\n",
    " 'Frontal_Inf_Orb_L', 'Frontal_Inf_Orb_R',\n",
    " 'Rolandic_Oper_L', 'Rolandic_Oper_R',\n",
    " 'Supp_Motor_Area_L', 'Supp_Motor_Area_R',\n",
    " 'Olfactory_L', 'Olfactory_R',\n",
    " 'Frontal_Sup_Medial_L', 'Frontal_Sup_Medial_R',\n",
    " 'Frontal_Med_Orb_L', 'Frontal_Med_Orb_R',\n",
    " 'Rectus_L', 'Rectus_R',\n",
    " 'Insula_L', 'Insula_R',\n",
    " 'Cingulum_Ant_L', 'Cingulum_Ant_R',\n",
    " 'Cingulum_Mid_L', 'Cingulum_Mid_R',\n",
    " 'Cingulum_Post_L', 'Cingulum_Post_R',\n",
    " 'Hippocampus_L', 'Hippocampus_R',\n",
    " 'ParaHippocampal_L', 'ParaHippocampal_R',\n",
    " 'Amygdala_L', 'Amygdala_R',\n",
    " 'Calcarine_L', 'Calcarine_R',\n",
    " 'Cuneus_L', 'Cuneus_R',\n",
    " 'Lingual_L', 'Lingual_R',\n",
    " 'Occipital_Sup_L', 'Occipital_Sup_R',\n",
    " 'Occipital_Mid_L', 'Occipital_Mid_R',\n",
    " 'Occipital_Inf_L', 'Occipital_Inf_R',\n",
    " 'Fusiform_L', 'Fusiform_R',\n",
    " 'Postcentral_L', 'Postcentral_R',\n",
    " 'Parietal_Sup_L', 'Parietal_Sup_R',\n",
    " 'Parietal_Inf_L', 'Parietal_Inf_R',\n",
    " 'SupraMarginal_L', 'SupraMarginal_R',\n",
    " 'Angular_L', 'Angular_R',\n",
    " 'Precuneus_L', 'Precuneus_R',\n",
    " 'Paracentral_Lobule_L', 'Paracentral_Lobule_R',\n",
    " 'Caudate_L', 'Caudate_R',\n",
    " 'Putamen_L', 'Putamen_R',\n",
    " 'Pallidum_L', 'Pallidum_R',\n",
    " 'Thalamus_L', 'Thalamus_R',\n",
    " 'Heschl_L', 'Heschl_R',\n",
    " 'Temporal_Sup_L', 'Temporal_Sup_R',\n",
    " 'Temporal_Pole_Sup_L', 'Temporal_Pole_Sup_R',\n",
    " 'Temporal_Mid_L', 'Temporal_Mid_R',\n",
    " 'Temporal_Pole_Mid_L', 'Temporal_Pole_Mid_R',\n",
    " 'Temporal_Inf_L', 'Temporal_Inf_R',])\n",
    "\n",
    "atlas = datasets.fetch_atlas_aal()\n",
    "print('Power atlas comes with {0}.'.format(atlas.keys()))\n",
    "all_location = []\n",
    "for iter_roi_name in roi_name:\n",
    "    location = np.where(np.array(atlas['labels']) == iter_roi_name)[0][0]\n",
    "    all_location.append(location)\n",
    "\n",
    "all_location = np.array(all_location)\n",
    "print(all_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到rest和task时的功能连接，并且只留下半矩阵并且铺平\n",
    "filename = ['task', 'rest']\n",
    "all_data = {}\n",
    "for iter_file in filename:\n",
    "\n",
    "    iter_file_data = {}\n",
    "\n",
    "    fmri_data = np.load(data_path + iter_file + '.npy', allow_pickle=True).item()  # subjects * time * aal rois\n",
    "\n",
    "    for iter_condition in condition_name:\n",
    "\n",
    "        iter_condition_coor_matrix = []\n",
    "        for iter_subject in (range(len(fmri_data[iter_condition]))):\n",
    "            # 数据维度为 times * 28 ROI\n",
    "            iter_subjectfmri_data = np.array(fmri_data[iter_condition])[iter_subject, all_location]\n",
    "            # 把数据维度转换为 ROI * times\n",
    "            iter_subjectfmri_data = np.transpose(iter_subjectfmri_data)\n",
    "\n",
    "\n",
    "            iter_data = iter_subjectfmri_data[all_location]\n",
    "            # ROI两两相关\n",
    "            iter_coor_matrix = np.corrcoef(iter_data)\n",
    "            # 去除对角线\n",
    "            # skip_diag_matrix = skip_diag_masking(iter_coor_matrix)\n",
    "            # 选取六个ifg区域\n",
    "            # iter_ifg_matrix = skip_diag_matrix[ifg_location]\n",
    "            iter_coor_matrix = np.tril(iter_coor_matrix, k=-1)\n",
    "            iter_coor_matrix = iter_coor_matrix.flatten()\n",
    "            iter_coor_matrix = iter_coor_matrix[iter_coor_matrix!=0]\n",
    "\n",
    "            iter_condition_coor_matrix.append(iter_coor_matrix)\n",
    "\n",
    "        iter_condition_coor_matrix = np.array(iter_condition_coor_matrix)\n",
    "        iter_file_data[iter_condition] = iter_condition_coor_matrix\n",
    "    all_data[iter_file] = iter_file_data\n",
    "all_data['roi_name'] = roi_name\n",
    "\n",
    "print(np.array(all_data['task']['OT']).shape)\n",
    "print(np.array(all_data['rest']['PL']).shape)\n",
    "all_data['rest'] = np.vstack((all_data['rest']['OT'], all_data['rest']['PL']))\n",
    "all_data['task'] = np.vstack((all_data['task']['OT'], all_data['task']['PL']))\n",
    "print(np.array(all_data['rest']).shape)\n",
    "# 找到每个边的命名，并且只取下半部分\n",
    "all_fc_name = []\n",
    "for iter_x_location in roi_name:\n",
    "    iter_x = []\n",
    "    for iter_y_location in roi_name:\n",
    "        name = iter_x_location+ '_'+iter_y_location\n",
    "        iter_x.append(name)\n",
    "    all_fc_name.append(iter_x)\n",
    "all_fc_name = np.array(all_fc_name)\n",
    "all_fc_name = np.tril(all_fc_name, k=-1)\n",
    "all_fc_name = all_fc_name.flatten()\n",
    "all_fc_name = all_fc_name[all_fc_name!='']\n",
    "print(all_fc_name.shape)\n",
    "#储存\n",
    "all_data['all_fc_name'] = all_fc_name\n",
    "\n",
    "# hm_make_dir(result_path)\n",
    "# np.save(result_path + '/' + 'aal_90roi_fc.npy', all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select significant sides (rest)  OT and PL\n",
    "# create label\n",
    "from tqdm import trange\n",
    "\n",
    "threshold = 0.01\n",
    "behavior_data = 'rt_std'\n",
    "selected_feature = {}\n",
    "selected_feature['OT'] = {}\n",
    "selected_feature['PL'] = {}\n",
    "\n",
    "# for rest\n",
    "all_significant_num = []\n",
    "for iter_fc in trange(all_data['rest'].shape[1]):\n",
    "\n",
    "    # 提取输入数据\n",
    "    y = np.array(behavior_df[behavior_data])[0:30]\n",
    "    x = all_data['rest'][0:30, iter_fc]\n",
    "\n",
    "    scipy_corr = scipy.stats.pearsonr(x, y)\n",
    "\n",
    "    if scipy_corr[1] < threshold:\n",
    "        all_significant_num.append(iter_fc)\n",
    "    # \n",
    "selected_feature['OT']['feature'] = all_data['rest'][0:30, np.array(all_significant_num)]\n",
    "selected_feature['OT']['feature_name'] = all_fc_name[np.array(all_significant_num)]\n",
    "\n",
    "print(selected_feature['OT']['feature_name'].shape)\n",
    "\n",
    "# Select significant sides (rest)   PL\n",
    "all_significant_num = []\n",
    "for iter_fc in trange(all_data['rest'].shape[1]):\n",
    "\n",
    "    # 提取输入数据\n",
    "    y = np.array(behavior_df[behavior_data])[30:]\n",
    "    x = all_data['rest'][30:,iter_fc]\n",
    "\n",
    "    scipy_corr = scipy.stats.pearsonr(x, y)\n",
    "\n",
    "    if scipy_corr[1] < threshold:\n",
    "        all_significant_num.append(iter_fc)\n",
    "    # \n",
    "selected_feature['PL']['feature'] = all_data['rest'][30:, np.array(all_significant_num)]\n",
    "selected_feature['PL']['feature_name'] = all_fc_name[np.array(all_significant_num)]\n",
    "print(selected_feature['PL']['feature_name'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行单次 OT svm  for OT\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "import scipy.io as scio  # 用于读取matlab格式的数据\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso()\n",
    "etn = ElasticNet()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# import data\n",
    "dataset = scaler.fit_transform(selected_feature['OT']['feature']) \n",
    "# dataset = selected_feature['OT']['feature']    \n",
    "\n",
    "y = np.array(behavior_df[behavior_data])[0:30]\n",
    "############################SVM+LOOVC################\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(dataset)\n",
    "\n",
    "predictlabel_list = []\n",
    "reallabel_list = []\n",
    "\n",
    "# C：C-SVC的惩罚参数C\n",
    "# kernel: linear, poly, rbf, sigmoid, precomputed\n",
    "C = 1\n",
    "clf = SVR(C=C, kernel='linear', gamma='auto', degree=6, epsilon = 0.02)\n",
    "# clf = lasso\n",
    "# 用留一法进行验证\n",
    "for train_index, test_index in loo.split(dataset):\n",
    "    # 分割训练集\n",
    "    X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "    # 分割测试集\n",
    "    Y_train, Y_test = y[np.array(train_index)], y[np.array(test_index)]\n",
    "    # 拟合\n",
    "    clf.fit(X_train, Y_train)\n",
    "    # 储存训练结果\n",
    "    predictlabel_list.append(list(clf.predict(X_test)))\n",
    "    reallabel_list.append(list(Y_test))\n",
    "reallabel_list = np.squeeze(reallabel_list)\n",
    "predictlabel_list = np.squeeze(predictlabel_list)\n",
    "\n",
    "scipy_corr = scipy.stats.spearmanr(reallabel_list, predictlabel_list)\n",
    "print('******循环结束！************')\n",
    "print('准确率为：',  (scipy_corr))\n",
    "print('******运行结束！************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#进行单次 OT svm for pl\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "import scipy.io as scio  # 用于读取matlab格式的数据\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# import data\n",
    "dataset = scaler.fit_transform(selected_feature['PL']['feature']) \n",
    "y = np.array(behavior_df[behavior_data])[30:]\n",
    "# dataset = selected_feature['PL']['feature']    \n",
    "############################SVM+LOOVC################\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(dataset)\n",
    "predictlabel_list_pl = []\n",
    "reallabel_list_pl = []\n",
    "\n",
    "# C：C-SVC的惩罚参数C\n",
    "# kernel: linear, poly, rbf, sigmoid, precomputed\n",
    "C = 1\n",
    "clf = SVR(C=C, kernel='linear', gamma='auto', degree=6, epsilon = 0.02)\n",
    "\n",
    "# 用留一法进行验证\n",
    "for train_index, test_index in loo.split(dataset):\n",
    "    # 分割训练集\n",
    "    X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "    # 分割测试集\n",
    "    Y_train, Y_test = y[np.array(train_index)], y[np.array(test_index)]\n",
    "    # 拟合\n",
    "    clf.fit(X_train, Y_train)\n",
    "    # 储存训练结果\n",
    "    predictlabel_list_pl.append(list(clf.predict(X_test)))\n",
    "    reallabel_list_pl.append(list(Y_test))\n",
    "reallabel_list_pl = np.squeeze(reallabel_list_pl)\n",
    "predictlabel_list_pl = np.squeeze(predictlabel_list_pl)\n",
    "# accurancy = count_right_label / len(label)\n",
    "scipy_corr = scipy.stats.spearmanr(reallabel_list_pl, predictlabel_list_pl)\n",
    "print('******循环结束！************')\n",
    "print('准确率为：',  (scipy_corr))\n",
    "print('******运行结束！************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result correlation OT\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,8),dpi=200)\n",
    "labelsize = 15\n",
    "ticksize = 18\n",
    "dot_size = 80\n",
    "# x = 'Caudate_R_Frontal_Mid_Orb_R'\n",
    "# y = 'acc_mean'\n",
    "sns.regplot(x=reallabel_list, y=predictlabel_list, color=\"#FC8D62\",marker=\"o\", scatter_kws={'s':dot_size}, line_kws={'linewidth':4})\n",
    "sns.despine()\n",
    "\n",
    "# plt.xlabel(x, fontsize=labelsize)\n",
    "# plt.ylabel(y, fontsize=labelsize)\n",
    "\n",
    "plt.yticks(size=ticksize, fontproperties='Arial')\n",
    "plt.xticks(size=ticksize, fontproperties='Arial')\n",
    "leg = plt.legend([])\n",
    "leg.get_frame().set_linewidth(0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result correlation PL\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,8),dpi=200)\n",
    "labelsize = 15\n",
    "ticksize = 18\n",
    "dot_size = 80\n",
    "# x = 'Caudate_R_Frontal_Mid_Orb_R'\n",
    "# y = 'acc_mean'\n",
    "sns.regplot(x=reallabel_list_pl, y=predictlabel_list_pl, color=\"#3E9C7F\",marker=\"o\", scatter_kws={'s':dot_size}, line_kws={'linewidth':4})\n",
    "sns.despine()\n",
    "\n",
    "# plt.xlabel(x, fontsize=labelsize)\n",
    "# plt.ylabel(y, fontsize=labelsize)\n",
    "\n",
    "plt.yticks(size=ticksize, fontproperties='Arial')\n",
    "plt.xticks(size=ticksize, fontproperties='Arial')\n",
    "leg = plt.legend([])\n",
    "leg.get_frame().set_linewidth(0.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old permutation test for OT\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import scipy.io as scio  # 用于读取matlab格式的数据\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# import data\n",
    "dataset = scaler.fit_transform(selected_feature['OT']['feature']) \n",
    "# dataset = selected_feature['OT']['feature']    \n",
    "\n",
    "y = np.array(behavior_df[behavior_data])[0:30]\n",
    "############################SVM+LOOVC################\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(dataset)\n",
    "\n",
    "predictlabel_list = []\n",
    "reallabel_list = []\n",
    "\n",
    "# C：C-SVC的惩罚参数C\n",
    "# kernel: linear, poly, rbf, sigmoid, precomputed\n",
    "C = 1\n",
    "clf = SVR(C=C, kernel='linear', gamma='auto', degree=6, epsilon = 0.02)\n",
    "# clf = lasso\n",
    "# 用留一法进行验证\n",
    "for train_index, test_index in loo.split(dataset):\n",
    "    # 分割训练集\n",
    "    X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "    # 分割测试集\n",
    "    Y_train, Y_test = y[np.array(train_index)], y[np.array(test_index)]\n",
    "    # 拟合\n",
    "    clf.fit(X_train, Y_train)\n",
    "    # 储存训练结果\n",
    "    predictlabel_list.append(list(clf.predict(X_test)))\n",
    "    reallabel_list.append(list(Y_test))\n",
    "reallabel_list = np.squeeze(reallabel_list)\n",
    "predictlabel_list = np.squeeze(predictlabel_list)\n",
    "# accurancy = count_right_label / len(label)\n",
    "scipy_corr = scipy.stats.spearmanr(reallabel_list, predictlabel_list)\n",
    "acc_0 = scipy_corr[0]\n",
    "print('******循环结束！************')\n",
    "print('准确率为：',  (scipy_corr))\n",
    "print('******运行结束！************')\n",
    "\n",
    "############################Permutation################\n",
    "n_permutation = 10000\n",
    "\n",
    "all_permutation_acc_ot = []\n",
    "for iter_n in trange(n_permutation):\n",
    "    # label permutation\n",
    "    iter_y = y.copy()\n",
    "    iter_y = np.random.permutation(iter_y)\n",
    "    ############################SVM+LOOVC################\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(dataset)\n",
    "    predictlabel_list = []\n",
    "    reallabel_list = []\n",
    "\n",
    "    # 用留一法进行验证\n",
    "    for train_index, test_index in loo.split(dataset):\n",
    "        # 分割训练集\n",
    "        X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "        # 分割测试集\n",
    "        Y_train, Y_test = iter_y[np.array(train_index)], iter_y[np.array(test_index)]\n",
    "        # 拟合\n",
    "        clf.fit(X_train, Y_train)\n",
    "        # 储存训练结果\n",
    "        predictlabel_list.append(list(clf.predict(X_test)))\n",
    "        reallabel_list.append(list(Y_test))\n",
    "\n",
    "    scipy_corr = scipy.stats.spearmanr(reallabel_list, predictlabel_list)\n",
    "    all_permutation_acc_ot.append(scipy_corr[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the permutation result for OT\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# acc_0 = 0.492\n",
    "bins_interval= 1/30\n",
    "bins =np.arange(min(all_permutation_acc_ot) - bins_interval/2, max(all_permutation_acc_ot)+bins_interval/2, bins_interval)\n",
    "# bins = int((max(all_permutation_acc) - min(all_permutation_acc))/bins_interval)+1\n",
    "p_value = sum(ms > (acc_0) for ms in all_permutation_acc_ot) / len(all_permutation_acc_ot)\n",
    "print(p_value)\n",
    "plt.figure(figsize=(4.5,3),dpi=100)\n",
    "ax=plt.gca()\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['left'].set_color('none')\n",
    "ax.spines['bottom'].set_color('none')\n",
    "# sns.kdeplot(all_permutation_acc, shade=True, color='salmon')\n",
    "# plt.hist(all_permutation_acc, color='salmon', bins = bins)\n",
    "sns.distplot(all_permutation_acc_ot, bins=bins, color='#FCC3AA',hist_kws={'rwidth': 0.7})   # kdeplot\n",
    "# plt.title('rbf 02 10k C = '+ str(C) + ',P = ' + str(p_value))\n",
    "# plt.xlabel('acc')\n",
    "# plt.xlim([-0.6,0.6])\n",
    "plt.axvline(acc_0, c=\"#FC8D62\", ls = \"dashed\",linewidth=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old permutation test for PL\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import scipy.io as scio  # 用于读取matlab格式的数据\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# import data\n",
    "dataset = scaler.fit_transform(selected_feature['PL']['feature']) \n",
    "# dataset = selected_feature['OT']['feature']    \n",
    "\n",
    "y = np.array(behavior_df[behavior_data])[30:]\n",
    "############################SVM+LOOVC################\n",
    "loo = LeaveOneOut()\n",
    "loo.get_n_splits(dataset)\n",
    "\n",
    "predictlabel_list = []\n",
    "reallabel_list = []\n",
    "\n",
    "# C：C-SVC的惩罚参数C\n",
    "# kernel: linear, poly, rbf, sigmoid, precomputed\n",
    "C = 1\n",
    "clf = SVR(C=C, kernel='linear', gamma='auto', degree=6, epsilon = 0.02 )\n",
    "# clf = lasso\n",
    "# 用留一法进行验证\n",
    "for train_index, test_index in loo.split(dataset):\n",
    "    # 分割训练集\n",
    "    X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "    # 分割测试集\n",
    "    Y_train, Y_test = y[np.array(train_index)], y[np.array(test_index)]\n",
    "    # 拟合\n",
    "    clf.fit(X_train, Y_train)\n",
    "    # 储存训练结果\n",
    "    predictlabel_list.append(list(clf.predict(X_test)))\n",
    "    reallabel_list.append(list(Y_test))\n",
    "reallabel_list = np.squeeze(reallabel_list)\n",
    "predictlabel_list = np.squeeze(predictlabel_list)\n",
    "# accurancy = count_right_label / len(label)\n",
    "scipy_corr = scipy.stats.spearmanr(reallabel_list, predictlabel_list)\n",
    "acc_2 = scipy_corr[0]\n",
    "print('******循环结束！************')\n",
    "print('准确率为：',  (scipy_corr))\n",
    "print('******运行结束！************')\n",
    "\n",
    "############################Permutation################\n",
    "n_permutation = 10000\n",
    "\n",
    "all_permutation_acc_pl = []\n",
    "for iter_n in trange(n_permutation):\n",
    "    # label permutation\n",
    "    iter_y = y.copy()\n",
    "    iter_y = np.random.permutation(iter_y)\n",
    "    ############################SVM+LOOVC################\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(dataset)\n",
    "    predictlabel_list = []\n",
    "    reallabel_list = []\n",
    "\n",
    "    # 用留一法进行验证\n",
    "    for train_index, test_index in loo.split(dataset):\n",
    "        # 分割训练集\n",
    "        X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "        # 分割测试集\n",
    "        Y_train, Y_test = iter_y[np.array(train_index)], iter_y[np.array(test_index)]\n",
    "        # 拟合\n",
    "        clf.fit(X_train, Y_train)\n",
    "        # 储存训练结果\n",
    "        predictlabel_list.append(list(clf.predict(X_test)))\n",
    "        reallabel_list.append(list(Y_test))\n",
    "\n",
    "    scipy_corr = scipy.stats.spearmanr(reallabel_list, predictlabel_list)\n",
    "    all_permutation_acc_pl.append(scipy_corr[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot permutation test for pl\n",
    "# acc_2 = 0.101\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "bins_interval= 1/29\n",
    "bins =np.arange(min(all_permutation_acc_pl) - bins_interval/2, max(all_permutation_acc_pl)+bins_interval/2, bins_interval)\n",
    "# bins = int((max(all_permutation_acc) - min(all_permutation_acc))/bins_interval)+1\n",
    "p_value = sum(ms > (acc_2) for ms in all_permutation_acc_pl) / len(all_permutation_acc_pl)\n",
    "print(p_value)\n",
    "plt.figure(figsize=(4.5,3),dpi=100)\n",
    "ax=plt.gca()\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['left'].set_color('none')\n",
    "ax.spines['bottom'].set_color('none')\n",
    "# sns.kdeplot(all_permutation_acc, shade=True, color='salmon')\n",
    "# plt.hist(all_permutation_acc, color='salmon', bins = bins)\n",
    "sns.distplot(all_permutation_acc_pl, bins=bins, color='#8ED2BD',hist_kws={'rwidth': 0.7})   # kdeplot\n",
    "# plt.title('rbf 02 10k C = '+ str(C) + ',P = ' + str(p_value))\n",
    "# plt.xlabel('acc')\n",
    "# plt.xlim([-0.6,0.6])\n",
    "plt.axvline(acc_2, c=\"#3E9C7F\", ls = \"dashed\",linewidth = 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new permutation from feature selection for PL\n",
    "# permutate from select features\n",
    "\n",
    "#进行单次 OT svm\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "import scipy.io as scio  # 用于读取matlab格式的数据\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "from tqdm import trange\n",
    "\n",
    "# basic information\n",
    "threshold = 0.01\n",
    "behavior_data = 'acc_mean'\n",
    "\n",
    "# 提取输入数据\n",
    "y_pl = np.array(behavior_df[behavior_data])[30:]\n",
    "x_pl = all_data['rest'][30:]\n",
    "\n",
    "# 创建回归器\n",
    "# C：C-SVC的惩罚参数C  epsilon：与学习率有关\n",
    "# kernel: linear, poly, rbf, sigmoid, precomputed\n",
    "clf = SVR(C=1, kernel='linear', gamma='auto', degree=6, epsilon = 0.1)\n",
    "\n",
    "#######################################Permutation##############################\n",
    "n_permutation = 10000\n",
    "\n",
    "# 创建变量存储每次排列的结果\n",
    "all_permutation_acc_pl = []\n",
    "for iter_n in trange(n_permutation):\n",
    "    # label permutation\n",
    "\n",
    "    iter_y_pl = y_pl.copy()\n",
    "    iter_y_pl = np.random.permutation(iter_y_pl)\n",
    "\n",
    "    # create dict to store selected feature\n",
    "    selected_feature = {}\n",
    "    selected_feature['PL'] = {}\n",
    "\n",
    "    ######################################select features #################################\n",
    "    \n",
    "    # print(selected_feature['OT']['feature_name'].shape)\n",
    "\n",
    "    # Select significant sides   PL\n",
    "    all_significant_num = []\n",
    "    for iter_fc in range(x_pl.shape[1]):\n",
    "\n",
    "        scipy_corr = scipy.stats.pearsonr(x_pl[:,iter_fc], iter_y_pl)\n",
    "\n",
    "        if scipy_corr[1] < threshold:\n",
    "            all_significant_num.append(iter_fc)\n",
    "        # \n",
    "    try:\n",
    "        selected_feature['PL']['feature'] = x_pl[:, np.array(all_significant_num)]\n",
    "        selected_feature['PL']['feature_name'] = all_fc_name[np.array(all_significant_num)]\n",
    "    except:\n",
    "        continue\n",
    "    # print(selected_feature['PL']['feature_name'].shape)\n",
    "\n",
    "\n",
    "    ###################################### SVM+LOOVC##################################\n",
    "   \n",
    "    # PL\n",
    "    dataset = selected_feature['PL']['feature']\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(dataset)\n",
    "    predictlabel_list = []\n",
    "    reallabel_list = []\n",
    "\n",
    "    # 用留一法进行验证\n",
    "    for train_index, test_index in loo.split(dataset):\n",
    "        # 分割训练集\n",
    "        X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "        # 分割测试集\n",
    "        Y_train, Y_test = iter_y_pl[np.array(train_index)], iter_y_pl[np.array(test_index)]\n",
    "        # 拟合\n",
    "        clf.fit(X_train, Y_train)\n",
    "        # 储存训练结果\n",
    "        predictlabel_list.append(list(clf.predict(X_test)))\n",
    "        reallabel_list.append(list(Y_test))\n",
    "\n",
    "    scipy_corr = scipy.stats.spearmanr(reallabel_list, predictlabel_list)\n",
    "    all_permutation_acc_pl.append(scipy_corr[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new permutation from feature selection for OT and PL\n",
    "# permutate from select features\n",
    "\n",
    "#进行单次 OT svm\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "import scipy.io as scio  # 用于读取matlab格式的数据\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "from tqdm import trange\n",
    "\n",
    "# basic information\n",
    "threshold = 0.01\n",
    "behavior_data = 'acc_mean'\n",
    "\n",
    "# 提取输入数据\n",
    "y_ot = np.array(behavior_df[behavior_data])[0:30]\n",
    "x_ot = all_data['rest'][0:30]\n",
    "\n",
    "y_pl = np.array(behavior_df[behavior_data])[30:]\n",
    "x_pl = all_data['rest'][30:]\n",
    "\n",
    "# 创建回归器\n",
    "# C：C-SVC的惩罚参数C  epsilon：与学习率有关\n",
    "# kernel: linear, poly, rbf, sigmoid, precomputed\n",
    "clf = SVR(C=1, kernel='linear', gamma='auto', degree=6, epsilon = 0.1)\n",
    "\n",
    "#######################################Permutation##############################\n",
    "n_permutation = 10000\n",
    "\n",
    "# 创建变量存储每次排列的结果\n",
    "all_permutation_acc_ot = []\n",
    "all_permutation_acc_pl = []\n",
    "for iter_n in trange(n_permutation):\n",
    "    # label permutation\n",
    "    iter_y_ot = y_ot.copy()\n",
    "    iter_y_ot = np.random.permutation(iter_y_ot)\n",
    "    iter_y_pl = y_pl.copy()\n",
    "    iter_y_pl = np.random.permutation(iter_y_pl)\n",
    "\n",
    "    # create dict to store selected feature\n",
    "    selected_feature = {}\n",
    "    selected_feature['OT'] = {}\n",
    "    selected_feature['PL'] = {}\n",
    "\n",
    "    ######################################select features #################################\n",
    "    # Select significant sides   OT\n",
    "    all_significant_num = []\n",
    "    for iter_fc in range(x_ot.shape[1]):\n",
    "\n",
    "        scipy_corr = scipy.stats.pearsonr(x_ot[:,iter_fc], iter_y_ot)\n",
    "\n",
    "        if scipy_corr[1] < threshold:\n",
    "            all_significant_num.append(iter_fc)\n",
    "        # \n",
    "    selected_feature['OT']['feature'] = x_ot[:, np.array(all_significant_num)]\n",
    "    selected_feature['OT']['feature_name'] = all_fc_name[np.array(all_significant_num)]\n",
    "\n",
    "    # print(selected_feature['OT']['feature_name'].shape)\n",
    "\n",
    "    # Select significant sides   PL\n",
    "    all_significant_num = []\n",
    "    for iter_fc in range(x_pl.shape[1]):\n",
    "\n",
    "        scipy_corr = scipy.stats.pearsonr(x_pl[:,iter_fc], iter_y_pl)\n",
    "\n",
    "        if scipy_corr[1] < threshold:\n",
    "            all_significant_num.append(iter_fc)\n",
    "        # \n",
    "    selected_feature['PL']['feature'] = x_pl[:, np.array(all_significant_num)]\n",
    "    selected_feature['PL']['feature_name'] = all_fc_name[np.array(all_significant_num)]\n",
    "    # print(selected_feature['PL']['feature_name'].shape)\n",
    "\n",
    "\n",
    "    ###################################### SVM+LOOVC##################################\n",
    "    # OT\n",
    "    dataset = selected_feature['OT']['feature']\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(dataset)\n",
    "    predictlabel_list = []\n",
    "    reallabel_list = []\n",
    "\n",
    "    # 用留一法进行验证\n",
    "    for train_index, test_index in loo.split(dataset):\n",
    "        # 分割训练集\n",
    "        X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "        # 分割测试集\n",
    "        Y_train, Y_test = iter_y_ot[np.array(train_index)], iter_y_ot[np.array(test_index)]\n",
    "        # 拟合\n",
    "        clf.fit(X_train, Y_train)\n",
    "        # 储存训练结果\n",
    "        predictlabel_list.append(list(clf.predict(X_test)))\n",
    "        reallabel_list.append(list(Y_test))\n",
    "\n",
    "    scipy_corr = scipy.stats.spearmanr(reallabel_list, predictlabel_list)\n",
    "    all_permutation_acc_ot.append(scipy_corr[0])\n",
    "\n",
    "    # PL\n",
    "    dataset = selected_feature['PL']['feature']\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(dataset)\n",
    "    predictlabel_list = []\n",
    "    reallabel_list = []\n",
    "\n",
    "    # 用留一法进行验证\n",
    "    for train_index, test_index in loo.split(dataset):\n",
    "        # 分割训练集\n",
    "        X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "        # 分割测试集\n",
    "        Y_train, Y_test = iter_y_pl[np.array(train_index)], iter_y_pl[np.array(test_index)]\n",
    "        # 拟合\n",
    "        clf.fit(X_train, Y_train)\n",
    "        # 储存训练结果\n",
    "        predictlabel_list.append(list(clf.predict(X_test)))\n",
    "        reallabel_list.append(list(Y_test))\n",
    "\n",
    "    scipy_corr = scipy.stats.spearmanr(reallabel_list, predictlabel_list)\n",
    "    all_permutation_acc_pl.append(scipy_corr[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new permutation from feature selection for OT\n",
    "# permutate from select features\n",
    "\n",
    "#进行单次 OT svm\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "import scipy.io as scio  # 用于读取matlab格式的数据\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "scaler = StandardScaler()\n",
    "from tqdm import trange\n",
    "\n",
    "# basic information\n",
    "threshold = 0.01\n",
    "behavior_data = 'acc_mean'\n",
    "\n",
    "# 提取输入数据\n",
    "y_ot = np.array(behavior_df[behavior_data])[0:30]\n",
    "x_ot = all_data['rest'][0:30]\n",
    "\n",
    "# 创建回归器\n",
    "# C：C-SVC的惩罚参数C  epsilon：与学习率有关\n",
    "# kernel: linear, poly, rbf, sigmoid, precomputed\n",
    "clf = SVR(C=1, kernel='linear', gamma='auto', degree=6, epsilon = 0.1)\n",
    "\n",
    "#######################################Permutation##############################\n",
    "n_permutation = 10000\n",
    "\n",
    "# 创建变量存储每次排列的结果\n",
    "all_permutation_acc_ot = []\n",
    "for iter_n in trange(n_permutation):\n",
    "    # label permutation\n",
    "    iter_y_ot = y_ot.copy()\n",
    "    iter_y_ot = np.random.permutation(iter_y_ot)\n",
    "\n",
    "    # create dict to store selected feature\n",
    "    selected_feature = {}\n",
    "    selected_feature['OT'] = {}\n",
    "\n",
    "    ######################################select features #################################\n",
    "    # Select significant sides   OT\n",
    "    all_significant_num = []\n",
    "    for iter_fc in range(x_ot.shape[1]):\n",
    "\n",
    "        scipy_corr = scipy.stats.pearsonr(x_ot[:,iter_fc], iter_y_ot)\n",
    "\n",
    "        if scipy_corr[1] < threshold:\n",
    "            all_significant_num.append(iter_fc)\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        selected_feature['OT']['feature'] = x_ot[:, np.array(all_significant_num)]\n",
    "        selected_feature['OT']['feature_name'] = all_fc_name[np.array(all_significant_num)]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    # print(selected_feature['OT']['feature_name'].shape)\n",
    "\n",
    "    ###################################### SVM+LOOVC##################################\n",
    "    # OT\n",
    "    dataset = selected_feature['OT']['feature']\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(dataset)\n",
    "    predictlabel_list = []\n",
    "    reallabel_list = []\n",
    "\n",
    "    # 用留一法进行验证\n",
    "    for train_index, test_index in loo.split(dataset):\n",
    "        # 分割训练集\n",
    "        X_train, X_test = dataset[train_index], dataset[test_index]\n",
    "        # 分割测试集\n",
    "        Y_train, Y_test = iter_y_ot[np.array(train_index)], iter_y_ot[np.array(test_index)]\n",
    "        # 拟合\n",
    "        clf.fit(X_train, Y_train)\n",
    "        # 储存训练结果\n",
    "        predictlabel_list.append(list(clf.predict(X_test)))\n",
    "        reallabel_list.append(list(Y_test))\n",
    "\n",
    "    scipy_corr = scipy.stats.spearmanr(reallabel_list, predictlabel_list)\n",
    "    all_permutation_acc_ot.append(scipy_corr[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每个特征都删一次,得到新的相关性\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import scipy.io as scio  # 用于读取matlab格式的数据\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# import data\n",
    "dataset = scaler.fit_transform(selected_feature['OT']['feature'])\n",
    "# dataset = selected_feature['rest']['feature']    \n",
    "y = np.array(behavior_df[behavior_data])[0:30]\n",
    "\n",
    "# C：C-SVC的惩罚参数C\n",
    "# kernel: linear, poly, rbf, sigmoid, precomputed\n",
    "C = 1\n",
    "clf = SVR(C=C, kernel='linear', gamma='auto', degree=6, epsilon = 0.02 )\n",
    "\n",
    "\n",
    "############################iter_feature################\n",
    "\n",
    "# 生成储存变量\n",
    "all_acc = []\n",
    "all_feature_p = []\n",
    "n_feature = dataset.shape[1]\n",
    "for iter_n in trange(n_feature):\n",
    "    # delete one feature \n",
    "    iter_dataset = dataset.copy()\n",
    "    iter_dataset = np.delete(iter_dataset, iter_n, axis=1)\n",
    "\n",
    "    ############################SVM+LOOVC################ 先进行一次，查看permutation前的正确率\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(iter_dataset)\n",
    "    predictlabel_list = []\n",
    "    reallabel_list = []\n",
    "\n",
    "    # 用留一法进行验证\n",
    "    for train_index, test_index in loo.split(iter_dataset):\n",
    "        # 分割训练集\n",
    "        X_train, X_test = iter_dataset[train_index], iter_dataset[test_index]\n",
    "        # 分割测试集\n",
    "        Y_train, Y_test = y[np.array(train_index)], y[np.array(test_index)]\n",
    "        # 拟合\n",
    "        clf.fit(X_train, Y_train)\n",
    "        # 储存训练结果\n",
    "        predictlabel_list.append(list(clf.predict(X_test)))\n",
    "        reallabel_list.append(list(Y_test))\n",
    "    # 生成留一法后的正确率\n",
    "    scipy_corr = scipy.stats.spearmanr(predictlabel_list, reallabel_list)\n",
    "\n",
    "    all_acc.append(scipy_corr[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据acc的差距展示重要的FC\n",
    "max_acc = 0.575\n",
    "acc_difference = max_acc - np.array(all_acc) \n",
    "\n",
    "sorted_index = np.argsort(acc_difference)\n",
    "sorted_p_difference = acc_difference[sorted_index]\n",
    "sorted_featurename = selected_feature['OT']['feature_name'][sorted_index]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,4),dpi=300)\n",
    "plt.barh(range(10), sorted_p_difference[-10:], tick_label=sorted_featurename[-10:], color='#4F83D9', height=0.6)\n",
    "# plt.xlabel('p-value difference')\n",
    "plt.xlabel('Feature importance (Acc difference)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_feature_p_value = np.load(result_path + '/' + 'all_feature_p_value.npy', allow_pickle=True).item()\n",
    "max_p = 0.492\n",
    "p_difference = all_feature_p_value['pvalue'] - max_p\n",
    "\n",
    "sorted_index = np.argsort(p_difference)\n",
    "sorted_p_difference = p_difference[sorted_index]\n",
    "sorted_featurename = all_feature_p_value['feature_name'][sorted_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,4),dpi=300)\n",
    "plt.barh(range(10), sorted_p_difference[-10:], tick_label=sorted_featurename[-10:], color='salmon', height=0.6)\n",
    "# plt.xlabel('p-value difference')\n",
    "plt.xlabel('feature importance (p-value difference)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(all_feature_p_value['acc']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importace_data_10 = pd.DataFrame()\n",
    "feature_importace_data_10['value'] = sorted_p_difference[-7:]\n",
    "feature_importace_data_10['brain_region1'] = ['Paracentral_Lobule_L', 'Temporal_Pole_Sup_L', \n",
    "                        'Insula_L', 'Putamen_R', 'Thalamus_R', 'Caudate_R', 'Temporal_Pole_Mid_R']\n",
    "feature_importace_data_10['brain_region2'] = [ 'Cingulum_Post_R', 'Angular_L', \n",
    "                        'Frontal_Mid_L', 'Frontal_Mid_Orb_R', 'Hippocampus_L', 'Frontal_Mid_Orb_R', 'Angular_L']\n",
    "feature_importace_data_10.to_csv(result_path + '/' + 'feature_importace_data_acc_7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_p_difference[-10:], tick_label=sorted_featurename[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保留指定行\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import scipy.io as scio  # 用于读取matlab格式的数据\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# import data\n",
    "dataset = scaler.fit_transform(selected_feature['rest']['feature'])\n",
    "# dataset = selected_feature['rest']['feature']    \n",
    "############################SVM#######################\n",
    "\n",
    "# C：C-SVC的惩罚参数C\n",
    "# kernel: linear, poly, rbf, sigmoid, precomputed\n",
    "C = 1\n",
    "clf = SVC(C=C, kernel='rbf', gamma='auto')\n",
    "\n",
    "############################Permutation################\n",
    "n_feature = dataset.shape[1]\n",
    "\n",
    "all_feature_acc = []\n",
    "\n",
    "all_acc = []\n",
    "select_feature = 30\n",
    "for iter_feature in range(select_feature):\n",
    "    # delete one feature \n",
    "    iter_dataset = dataset.copy()\n",
    "    # iter_dataset = np.delete(iter_dataset, sorted_index[0:1], axis=1)\n",
    "    iter_dataset = iter_dataset[:,sorted_index[-iter_feature:]]\n",
    "    ############################SVM+LOOVC################\n",
    "    loo = LeaveOneOut()\n",
    "    loo.get_n_splits(iter_dataset)\n",
    "    predictlabel_list = []\n",
    "    reallabel_list = []\n",
    "\n",
    "    # 用留一法进行验证\n",
    "    for train_index, test_index in loo.split(iter_dataset):\n",
    "        # 分割训练集\n",
    "        X_train, X_test = iter_dataset[train_index], iter_dataset[test_index]\n",
    "        # 分割测试集\n",
    "        Y_train, Y_test = label[np.array(train_index)], label[np.array(test_index)]\n",
    "        # 拟合\n",
    "        clf.fit(X_train, Y_train)\n",
    "        # 储存训练结果\n",
    "        predictlabel_list.append(list(clf.predict(X_test)))\n",
    "        reallabel_list.append(list(Y_test))\n",
    "\n",
    "    iter_accuracy = accuracy_score(reallabel_list,predictlabel_list)\n",
    "    # print(iter_accuracy)\n",
    "    all_acc.append(iter_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('data_process')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e42a79a1870fa07565d5bc2e72f1621e5b43aa8aa9140a304683a047737fef4c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
